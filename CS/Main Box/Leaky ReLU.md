---
aliases: []
---
## Content
Leaky ReLU is an [[Non-linear activation function|activation function]]
$$
\phi(x) = \left\{
\begin{array}{ll}
      x  & \text{if } x \gt 0 \\
      0.1x  & \text{otherwise}\\
\end{array} 
\right.
$$
## Tags
#external 

## Source
[[Redmon et al. (2016-b)]] - I don't think this is the first paper that introduced it.