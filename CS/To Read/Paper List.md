# SELF SUPERVISED 
* Masked Autoencoders Are Scalable Vision Learners (2021)
- Emerging Properties in Self-Supervised Vision Transformers (2021)
- VICREG: Variance-Invariance-Covariance Regularization for Self-Supervised Learning
- Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (2020)
- Momentum Contrast for Unsupervised Visual Representation Learning  (2020)
- Improved Baselines with Momentum Contrastive Learning (2020)
- Revisiting Self-Supervised Visual Representation Learning (2019)
- Local Aggregation for Unsupervised Learning of Visual Embeddings (2019)
- Representation learning with contrastive predictive coding (2018)
### Less known
- Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting 
-  [On Compositions of Transformations in Contrastive Self-Supervised Learning](https://arxiv.org/pdf/2003.04298.pdf)
- View Assignments with Support Samples
- [Mine your own view: Self-supervised learning through across-sample prediction](https://arxiv.org/abs/2102.10106)

# THEORY 
* Formal Algorithms for Transformers
* [Shortcut Learning in Deep Neural Networks](https://www.nature.com/articles/s42256-020-00257-z?ref=https://giter.site)
* Neural Networks and the Chomsky Hierarchy (DeepMind)
* Understanding deep learning requires rethinking generalization
* [When Does Contrastive Visual Representation Learning Work?](https://openaccess.thecvf.com/content/CVPR2022/papers/Cole_When_Does_Contrastive_Visual_Representation_Learning_Work_CVPR_2022_paper.pdf)
* Train faster, generalize better: Stability of stochastic gradient descent
* Bag of Tricks for Image Classification with Convolutional Neural Networks
- Identity Mappings in Deep Residual Networks
- Fortuitous Forgetting in Connectionist Networks (2022)
- The power of deeper networks for expressing natural functions (MIT)
- Deep, Skinny Neural Networks are not Universal Approximators
- Averaging Weights Leads to Wider Optima and Better Generalization
- The Computational Limits of Deep Learning
* Understanding deep learning requires rethinking generalization
* Formal Algorithms for Transformers
* Rethinking ImageNet Pre-training

# TRAINING
* When Does Label Smoothing Help?
- ResNet strikes back: An improved training procedure in timm
- Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation
- Scaling Vision Transformers
- ImageNet Training in Minutes
- LocoProp: Enhancing BackProp via Local Loss Optimization
 
# ARCHITECTURES
* Wide Residual Networks
- mobilenet II
- mobilenet III
- mobilenext
- xception
- mlpmixer
- regnet
- swin transformers
- Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions
- Genetic CNN
* AmoebaNet
* Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark

#### Architecture Search
- Nas
- MNasNet

# EXPLAINABLE AI
* [Deep Feature Interpolation for Image Content Changes](https://arxiv.org/pdf/1611.05507.pdf)
* [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)
* [Network-to-Network Translation with Conditional Invertible Neural Networks](https://arxiv.org/abs/2005.13580)
* BlobGAN: Spatially Disentangled Scene Representations.

# SEMANTIC SEGMENTATION
- [deeplab v3+](https://arxiv.org/pdf/1802.02611.pdf)
- A Probabilistic U-Net for Segmentation of Ambiguous Images

# NORMALIZATION
- How Does Batch Normalization Help Optimization?
- Rethinking Normalization and Elimination Singularity in Neural Networks

# MEDICAL
* Deep learning in cancer pathology: a new generation of clinical biomarkers
* Deep learning in digital pathology image analysis: a survey

# REINFORCEMENT LEARNING
* The related work in ***DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION*** seems to be very good.

# BOOKS
* Book: [The Principles of Deep Learning Theory](https://arxiv.org/pdf/2106.10165.pdf). It has very cool insights about why networks learn and talk about some recent things like residual nets.


# SHAPE/TEXTURE BIAS
* Reducing Domain Gap by Reducing Style Bias
* Reducing Domain Gap via Style-Agnostic Networks
* Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization
* Increasing Shape Bias in ImageNet-Trained Networks Using Transfer Learning and Domain-Adversarial Methods
* StyleAugment: Learning Texture De-biased Representations by Style Augmentation without Pre-defined Textures
* Contributions of Shape, Texture, and Color in Visual Recognition


# DIFFUSION MODELS
* Diffusion Models: A Comprehensive Survey of Methods and Applications


# ALGORITHMS
Â * Learning with Differentiable Algorithms


# GRAPH NEURAL NETWORKS
* [Graph Attention Networks](https://arxiv.org/abs/1710.10903)


# UNSPECIFIED
* OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks
* Alphafold 

# LISTS
* [aman paper](https://aman.ai/papers/)
